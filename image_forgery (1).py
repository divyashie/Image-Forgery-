# -*- coding: utf-8 -*-
"""Image_Forgery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16nQRe5JIKweTgL29e7YbQk2YqTzL7_rS
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import cv2
import glob
import numpy as np 
import pandas as pd
import sklearn 
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import random

from keras.preprocessing import image
from sklearn.datasets import load_files
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical

from keras.models import Sequential 
from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation 
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.constraints import maxnorm 
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint, TensorBoard

"""#### Step 1: Understand, Explore, and Visualize data"""

corpus = os.path.join("gdrive/Shared drives/Project9/datasets", "MICC-F2000")
images = []
for file in os.listdir(corpus): 
  if file.endswith(".jpg"): #removes the one .txt file 
      images.append(file)
print(sorted(set(images)))
print(len(images)) #2003 images, therefore duplication of file 

#remove duplication 
count = 0 
for img in images:
    if "(1).jpg" in img:
      count += 1  
      images.remove(img)
print(count)
print(len(set(images)))

"""Total number of Fake Images with .jpg 

---

format: *2001*

#### Step 2: Create partition for tampered, scaled
"""

y = []
tampered = [] 
scaled = [] 
others = []
for i in images: 
  if "tamp" in i: 
      tampered.append(i)
      y.append(1)
  elif "scale" in i: 
        scaled.append(i)
        y.append(0)
  else: 
      others.append(i)

print(len(scaled))
print(len(tampered))
print(len(others)) #check if there are others available

"""We have 1300 scaled images and 701 tampered images. Since they are all .jpg files, it makes sense to have more scaled than tampered files."""

#Understand Scaled up images 
def visualize(photo): 
    photo = plt.imread(str(corpus) + "/" + photo)
    dims = np.shape(photo)
    print(dims)
    print(np.min(photo), np.max(photo))
    #Collapse spatial dimensions to have a matrix of pixels by color channels
    pixel_matrix = np.reshape(photo, (dims[0]*dims[1], dims[2]))
    print(np.shape(pixel_matrix))
    plot_ = plt.hist2d(pixel_matrix[:,1], pixel_matrix[:,2], bins=(50,50))

visualize(tampered[53])

visualize(tampered[23])

visualize(scaled[11])

visualize(scaled[65])

"""#### Step 3: Preparing data for model

We need to prepare the datasets as tensor data versus list data since keras accept that. First, we have to normalize it with a target size. Each image will have its own dimension.

def preprocess_data(images):
  train_images = []
  for each_image in images: 
    img_path = corpus + "/" + each_image 
    img = imread(img_path, target_size=(150,150))
    img = np.array(img)
    #img_tensor = image.img_to_array(img)
    #img_tensor = np.expand_dims(img_tensor, axis=0)
    #img_tensor /= 255  #for normalization 
    img /= 255
    #train_images.append(img_tensor.shape)
    train_images.append(img)
  return train_images
"""

def preprocess(images): 
  IMG_SIZE = 224
  training_data = []
  for each_image in images:
    img_path = corpus + "/" + each_image
    img_array = cv2.imread(img_path)
    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
    #new_array /= 255
    training_data.append(new_array)
  return training_data

"""### Classification result"""

X = preprocess(images)
X = np.array(X) 
X = X.astype('float32') / 255
y = np.array(y)

type(X)

print(len(X))
print(len(y))

X.shape

"""####Step 4: Train test split

Since we have 1300 scaled images and 700 tampered images with no timestamp, therefore we will proceed with random splitting, i.e, 80% train and 20% test.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=5)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#Need to use one hot encoding
#y_train = np_utils.to_categorical(y_train)
#y_test = np_utils.to_categorical(y_test)
#class_num = y_test.shape[1]
#print(class_num)

"""#### Step 5: Create model"""

#CNN model 
model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = 5, padding = 'same', activation = 'sigmoid', input_shape = X.shape[1:]))
model.add(MaxPooling2D(pool_size = 2, strides = 2))
    #model.add(Dropout(0.4))
model.add(Conv2D(filters = 64, kernel_size = 5, padding = 'same', activation = 'sigmoid'))
model.add(MaxPooling2D(pool_size = 2, strides = 2))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(128, activation = 'sigmoid'))
model.add(Dense(50, activation='sigmoid'))
model.add(Dense(1, activation = 'softmax'))

model.summary()

"""#### Compile keras model"""

#model.compile(loss='binary_crossentropy', optimizer = 'adam', metric=['accuracy'])
#model.fit(X_train, y_train, epochs=150, batch_size=10)
model.compile(optimizer = 'adam',loss = 'binary_crossentropy', metrics = ['accuracy'])

model.fit(X_train, y_train, epochs=50, batch_size = 1)

"""#### Evaluate model"""

test_loss, test_acc = model.evaluate(X_test, y_test)

print("Test accuracy: ", test_acc)

